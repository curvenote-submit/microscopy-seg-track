---
title: 'Deep Learning Applications in Microscopy: Segmentation and Tracking'
description: ''
authors:
  - name: Yifei Duan
    corresponding: false
    roles: []
    affiliations:
      - >-
        Department of Materials Science and Engineering, University of
        Pennsylvania
  - name: Yifan Duan
    corresponding: false
    roles: []
    affiliations:
      - >-
        Department of Materials Science and Engineering, University of
        California, Berkeley
  - name: Zequn He
    corresponding: false
    roles: []
    affiliations:
      - >-
        Department of Materials Science and Engineering, University of
        Pennsylvania
  - name: Cheng-Yu Chen
    corresponding: false
    roles: []
    affiliations:
      - >-
        Department of Materials Science and Engineering, University of
        Pennsylvania
  - name: Eric A. Stach
    corresponding: false
    roles: []
    affiliations:
      - >-
        Department of Materials Science and Engineering, University of
        Pennsylvania
date: '2024-08-17'
keywords:
  - Deep learning
  - transmission electron microscopy
  - image segmentation
  - bright field transmission electron microscopy
  - nanoparticles
---

## Abstract

This paper reports the application of deep learning techniques in bright-field transmission electron microscopy image segmentation and tracking, in order to understand dynamic changes of nanoparticles during high-temperature sintering. Four state-of-the-art deep learning models, YOLOv8n-seg, Swin- UNet, VMamba, and EfficientSAM-tiny, were used and their performances were compared. The results show that EfficientSAM-tiny performs best in the segmentation task, achieving the highest accuracy (IoU 0.99672, Dice Coefficient 0.99836). In the tracking task, combining EfficientSAM-tiny with the DeAOT model successfully achieves efficient tracking and accurate identification of nanoparticles in microscope videos. The effectiveness and reliability of the method is verified by analyzing the video of the nanoparticle sintering process at a high temperature of 800Â°C. These results demonstrate the potential of deep learning techniques in microscope image analysis and introduce a new method from computer vision for microscope video tracking and analysis in materials science.

# Introduction

In recent years, deep learning techniques have shown great potential in microscopy image analysis, especially in the segmentation of high-resolution transmission electron microscopy (TEM) images. Deep learning methods can automatically extract useful information and effectively reveal the connection between microstructure and material properties {cite:p}`horwathUnderstandingImportantFeatures2020a`. As {cite:t}`geDeepLearningAnalysis2020` point out in their review, deep learning applied to microscopy image analysis establishes a generalized workflow, derived from the development of research in computer vision. Not only for electron microscopy but also for a wider range of analytical and imaging techniques can be applied to the same generalized workflow, such as light microscopy, scanning electron microscopy, atom probe tomography, and scanning probe microscopy.

Traditional analysis suffers from low efficiency. Machine learning models have faster processing speeds and handle larger amounts of data with impressive accuracy {cite:p}`horwathUnderstandingImportantFeatures2020a; geDeepLearningAnalysis2020`. For example, traditional analytical methods cannot handle the large amount of data generated by advanced microscopy methods such as in situ electron microscopy {cite:p}`geDeepLearningAnalysis2020`. For different types of microscopic images (morphology, size, distribution, intensity, etc.) at different scales, it is difficult to generalize traditional methods to the new data {cite:p}`geDeepLearningAnalysis2020`, such as sorting filters, Gaussian smoothing filters, histogram thresholding {cite:p}`tobiasImageSegmentationHistogram2002`, and edge detection {cite:p}`senthilkumaranImageSegmentationSurvey2009`, region growing {cite:p}`freixenetAnotherSurveyImage2002`. Often, the information extraction of these traditional algorithms tends to rely on specific features of a particular dataset {cite:p}`geDeepLearningAnalysis2020`. Furthermore, these methods rely on human experts to extract and set features based on their understanding of the dataset, thus being constrained by human limitations {cite:p}`ghoshUnderstandingDeepLearning2019`.

In the field of image processing and computer vision, image segmentation is the process of dividing a digital image into segments to make it more meaningful and easier to analyze, as the representation of the image is simplified by the division of the segments {cite:p}`shapiroComputerVision2001a`. The entire image is segmented into mutually overlapping segments, and each pixel within these regions is similar in some feature or computational attribute (e.g., color, intensity, or texture), whereas neighboring regions are significantly different in the same feature or computational attribute {cite:p}`nielsenRegionMergingStatistical2003; shapiroComputerVision2001a`. Image segmentation is one of the common computer vision tasks that has been applied in fields such as self-driving cars, video surveillance, machine perception, biomedical image analysis, and image understanding {cite:p}`minaeeImageSegmentationUsing2022; zhangResearchAdvancedImage2022`. For example, in medicine, it is used to recognize anatomical structures such as lesions or organs {cite:p}`minaeeImageSegmentationUsing2022`. In video surveillance, it can be used for land and city management {cite:p}`minaeeImageSegmentationUsing2022`.

In the field of materials science, it can be used to identify and analyze the microstructure of materials, such as defect analysis or component identification in ceramics or metals {cite:p}`geDeepLearningAnalysis2020; horwathUnderstandingImportantFeatures2020a`. {cite:t}`azimiAdvancedSteelMicrostructural2018` used a convolutional neural network called max-voted FCNN to recognize phases with different compositions in mild steel. Computer vision techniques can also be used to recognize two-dimensional materials in microscope images. In the study of {cite:p}`ramezaniAutomatic2DMaterial2023`, hexagonal boron nitride (hBN) flakes were automatically detected and classified, greatly improving analysis efficiency and reducing human intervention. These approaches are often quite generalizable, allowing the extensibility to other imaging modalities. In another study, {cite:t}`Masubuchi2020Deep` demonstrated a deep learning-based image segmentation algorithm for microscopy that automatically searches for 2D materials and achieves robust detection under multiple microscope conditions. Other types of microscopes, such as atomic force microscopes, can use image segmentation to extract valid information. In a research paper {cite:p}`zhuDeepLearningFrameworkAutomated2022`, deep learning frameworks such as Mask R-CNN were able to automatically perform scanning probe microscopy image analysis, which improved the accuracy and efficiency of molecule recognition.

The main goal of microscopy image segmentation is to accurately separate different regions or phases of an image, which is essential for further analysis and research. Conventional algorithms require tuning the parameters of the algorithm, which are often related to specific microscopy dataset properties. When measuring under different conditions, or switching to other microscopes, the parameters may need to be retuned, which is not generalizable {cite:p}`Masubuchi2020Deep`. For example, when the differences in the images are not obvious, the classical thresholding method makes it difficult to adjust the parameters to obtain accurate segmentation {cite:p}`sterbentzUniversalImageSegmentation2021`. The quality of each segmentation depends on the specific experimental conditions at hand. Another problem is that noise and artifacts often accompany microscopic images, and traditional algorithms may lead to segmentation errors {cite:p}`chuangObjectiveEvaluationSegmentation2010`.

The main contributions of this paper are as follows: 1. Four advanced segmentation models were compared by applying to material microscopy. Various neural network architectures, models, and pre-trained resources from the computer vision (CV) field were applied to material microscopy images. 2. We introduce the analysis of the time dimension. While existing microscope image segmentation methods for material science focus on the processing of static images, this paper introduces the analysis of the time dimension and realizes the tracking of different phases in microscope images in the time dimension.

This paper is divided into two parts. In the first part, we segment the different phases in a microscope image into masks. In the second part, we track the change of segmented masks in the time dimension. The first part applies state-of-the-art architectures and algorithms in computer vision to microscope image segmentation for materials science research. It compares four different deep neural models in computer vision. These include YOLOv8 {cite:p}`solawetzWhatYOLOv8Ultimate2023` based on Convolutional Neural Networks (CNN) architecture {cite:p}`lecunGradientbasedLearningApplied1998`, EfficientSAM {cite:p}`xiongEfficientSAMLeveragedMasked2023` based on Vision Transformer (ViT) {cite:p}`dosovitskiyIMAGEWORTH16X162021`, Swin-UNet {cite:p}`caoSwinUnetUnetlikePure2021; zhuSwinVoxelMorphSymmetricUnsupervised2022` based on Swin-Transformer {cite:p}`liuSwinTransformerHierarchical2021`, and VMamba-UNET {cite:p}`zhangVMUNETV2RethinkingVision2024` based on selective state spaces {cite:p}`guMambaLinearTimeSequence2023`. In the second part, the result of segmentation will be used as the input of the tracking model, which can realize efficient and accurate tracking for different phases and ensure the consistency of phase IDs through the EfficientSAM and DeAOT models.

# Methodology

## Algorithms from Computer Vision

The development of neural networks originated from the perceptron, introduced by {cite:t}`mccullochLogicalCalculusIdeas1943`. They are a computational model that mimics the neurons of the brain and consists of multiple connected nodes (neurons) distributed in the input, hidden, and output layers {cite:p}`bishopPatternRecognitionMachine2006`. A neural network transmits and processes information through weight and bias to achieve complex nonlinear mapping of input data. Trained by back-propagation, the neural network continuously adjusts weight and bias to learn from the data and make predictions or classifications {cite:p}`vapnikNatureStatisticalLearning1995`. While traditional image processing methods require manual design of feature extraction algorithms, neural networks, on the other hand, can automatically learn and extract features from images, thus reducing manual intervention and design complexity {cite:p}`lecunBackpropagationAppliedHandwritten1989`. With the development of machine learning research, various neural network architectures have been developed, including Convolutional Neural Networks (CNN) {cite:p}`lecunGradientbasedLearningApplied1998` Autoencoders {cite:p}`hintonReducingDimensionalityData2006`, Transformer {cite:p}`vaswaniAttentionAllYou2017`, and structured state space models (SSM) {cite:p}`guMambaLinearTimeSequence2023`.

- CNNs are a LeNet model first proposed by {cite:t}`lecunGradientbasedLearningApplied1998`. CNNs have become one of the core technologies of computer vision by effectively capturing local correlation patterns of images through convolutional layers. The convolutional layers pass through layer by layer, and the abstract features are automatically extracted from the convolutional kernel used to analyze the image {cite:t}`lecunBackpropagationAppliedHandwritten1989`. Then the CNN is used as part of the network components for feature extraction, and can be combined to build more complex neural networks to handle more complex tasks.
- Another neural network architecture is the autoencoder, which was proposed by {cite:t}`kramerNonlinearPrincipalComponent1991` . Autoencoders can be thought of as compressing and decompressing the dimensionality of the input information {cite:p}`kramerNonlinearPrincipalComponent1991; hintonReducingDimensionalityData2006`. As a modification of the autoencoder architecture, a U-shaped network (U-Net) was initially used to improve the performance of biomedical image segmentation by introducing a more efficient âSkip Connectionsâ design, as introduced by {cite:t}`ronnebergerUNetConvolutionalNetworks2015`. Good segmentation results can be achieved on very small training sets. âSkip Connectionsâ can significantly improve the training performance of networks, especially in very deep network structures {cite:p}`liVisualizingLossLandscape2018`.
- The âTransformerâ architecture is widely used in todayâs state-of-the-art Large Language Models (LLMs), such as LLaMa (open source) published by {cite:t}`touvronLLaMAOpenEfficient2023` and ChatGPT (closed source). The âTransformerâ architecture has been adapted for image processing in different ways. An architecture called Vision Transformer (ViT) was proposed by {cite:t}`dosovitskiyIMAGEWORTH16X162021`. In this approach, images are divided into 16Ã16 patches for input to the model. Another new variant, the Swin-Transformer, was proposed by Microsoft Research {cite:p}`liuSwinTransformerHierarchical2021`. It introduces a âshifted windowâ based on ViT, which is mainly used for other computer vision tasks. Swin-Transformer methods significantly reduce computational requirements by applying the self-attention mechanism only in a small window, which improves the efficiency of the model in processing large images.
- {cite:t}`guEfficientlyModelingLong2022` proposed the Structured State Space sequence model (S4), which is based on the SSM (State Space sequence model). A paper called Mamba in 2023 based on the S4 model has received a lot of attention {cite:p}`guMambaLinearTimeSequence2023`. Its main advantage is that it can process long sequences efficiently and achieves linear scaling of sequence length, in contrast to the Transformer architecture {cite:p}`liuVisionMambaComprehensive2024`. The reason Mamba can efficiently handle long sequences is due to its architecture, which differs from that of the Transformer and thus eliminates the limitations imposed by embedding in the Transformer architecture on the context window {cite:p}`xuVisualMambaSurvey2024`. For ultra-long sequences, such as videos, the Mamba model offers potential advantages. In the case of Transformers, which have a time complexity that scales quadratically, an increase in sequence length results in a computational cost that is significantly higher compared to the linear time complexity of the Mamba model {cite:p}`liuVisionMambaComprehensive2024`.

## Segmentation Models

In this paper, several segmentation models are compared, including YOLOv8n-seg, Swin-UNet, EfficientSAM-tiny, and VMamba. You Only Look Once (YOLO) is based on a CNN architecture {cite:p}`vRealTimeObject2022; congReviewYOLOObject2023; zhangYOLOSeriesTarget2023`. YOLO predicts multiple objects and categories in an image with a single forward pass and is suitable for speed-demanding applications. YOLO is designed for target detection tasks. Unlike earlier CNNs, YOLO is not only used for category probability recognition but also designed to give the box of a target {cite:p}`vRealTimeObject2022`. Swin-UNet by {cite:t}`zhuSwinVoxelMorphSymmetricUnsupervised2022` is a U-shaped Encoder-Decoder model based on a Transformer architecture that utilizes a hierarchical âSwin Transformerâ and a âShifted Windowâ for feature extraction, and then segments the image. VMamba-UNET {cite:p}`zhangVMUNETV2RethinkingVision2024` is a visual segmentation model implemented on top of the Mamba architecture based on the Visual State Space (VSS) module. The Mamba architecture is still controversial, and the low throughput of this model is evident in the results below. Segment Anything Model (SAM) by {cite:t}`kirillovSegmentAnything2023` is a generalized model trained by introducing a prompt segmentation task and a large segmentation dataset (SA-1B). SAM is inspired by ChatGPT, which enables zero-shot transfer of new image distributions and tasks (without the need to train on a specific class of objects) through prompt engineering techniques. SAM is a pre-training model, whose main strengths lie in its generalizability (zero-shot) and low data requirements (small amount of data for fine-tuning). SAM was trained using an extremely large dataset, SA-1B, which was expanded several times using the generated data.

## Tracking Model

The DeAOT model is a video tracking model, which gives image segmentation (spatial dimension) and ID matching (temporal dimension) to two networks for learning in decoupled networks {cite:p}`yangDecouplingFeaturesHierarchical2022`. The idea of decoupling has been used in several articles {cite:p}`chengSegmentTrackAnything2023; yangDecouplingFeaturesHierarchical2022; yanBridgingGapEndtoend2023; yangTrackAnythingSegment2023; zhangDVISDecoupledVideo2023`. This approach gives better video segmentation results than feeding this information to a single model. The DeAOT model also uses the Long Short-term TRansformer (LSTR) architecture {cite:p}`xuLongShortTermTransformer2021`. The advantages of introducing Long Short-Term Memory into the transformer model include the fact that the problem of vanishing gradients in loops {cite:p}`article` is solved, and historical information (long-term memory) is compressed and preserved {cite:p}`xuLongShortTermTransformer2021`. Meanwhile, for the tracking task, selectively outputting the current state information helps to effectively maintain long-term dependencies, thus achieving better tracking results over time. Since models are trained on segmentation datasets, they generally perform poorly in terms of maintaining temporal continuity of segmentation performance in videos. The use of a segmentation model combined with a tracking framework can enhance performance in video tracking tasks. Using the SAM model to generate the mask of some keyframes as a reference for DeAOT significantly improves the tracking performance of the model {cite:p}`chengSegmentTrackAnything2023`. However, a different approach is used in this paper to combine DeAOT with EfficientSAM. Specifically, the EfficientSAM segmented masks are used as information inputs for the DeAOT model. This approach is to improve the accuracy of model segmentation.

```{figure} images/HHKqdrccWdasix2fepBV-v5.png
:name: oxlKu6dMaY
:align: center
:width: 80%

Model pipeline diagram. Each frame is processed by a segmenter to generate a segmentation ID mask, followed by a tracker to update object IDs and produce a tracking ID mask.
```

{numref}`Figure %s <oxlKu6dMaY>` shows the segmentation and tracking process used in microscopic video analysis. For each frame, the image is first processed through the Segmenter to generate a âMask with Track IDâ, where each segmented region in the schematic is identified with a different color. In the initial frame, the segmentation result is added as a reference for the Tracker in the next frame. Starting from the second frame, the Segmenter continues to process each frame to generate the corresponding âTrack-IDâ. The Tracker then receives these âTrack-IDâ and updates the object IDs based on the information from the previous frame to generate the âMask with Track ID (Track-ID)â. This process is repeated in each frame of the video. The segmentation and tracking results of each frame are used as a reference for the tracking of the next frame, thus ensuring continuous tracking and accurate identification of the object. This pipeline enables efficient microscopic video analysis by combining the segmentation and tracking results in each frame. The segmentation information provided by the Tracker enables the tracker to more accurately identify and track dynamically changing targets.

```{figure} images/q3vNrXrLmInGjTQusIgj-v1.png
:name: VSiaa7L7sH
:align: center
:width: 70%
```

Algorithm 1 is the pseudocode for the flow in {numref}`Figure %s <oxlKu6dMaY>`. First, the segmenter and tracker are initialized. For each frame in the video, if it is the first frame, a prediction mask is generated by the segmenter and this mask is added as a reference for the tracker. For subsequent frames, a segmentation mask is generated by the segmenter and the segmentation mask is tracked by the tracker to generate a tracking mask. At the same time, a new object mask is detected. The tracking mask and the new object mask are merged to generate a prediction mask, and this prediction mask is added as a reference for the tracker. Through the above steps, each frame is segmented and tracked to ensure continuous tracking and accurate recognition of the target object. This approach improves the robustness and reliability of the system and is suitable for long-time tracking and analysis of microstructures.

## Model Information and Training Data

```{list-table} Models name, number of parameters, and resolutions of models in this paper.
:header-rows: 1
:name: vwwdoR5dj5

* - Models name

  - \# of parameters

  - Resolution

* - YOLOv8n-seg

  - 3,409,968

  - 512

* - Swin-UNet

  - 3,645,600

  - 448

* - VMamba

  - 3,145,179

  - 512

* - EfficientSAM-tiny

  - 10,185,380

  - 1024

```

{numref}`Table %s <vwwdoR5dj5>` shows the names and number of parameters of the four different models in this paper. Specifically, the YOLOv8n-seg model has 3,409,968 parameters; the Swin-UNet model has 3,645,600 parameters; and the VMamba model has 3, 145,179 parameters. In this paper, a distilled version of SAM is used, with 10,185,380 parameters. The high computational requirements of SAM limit wider applications, so there are many studies focusing on the distillation of SAM, such as FastSAM {cite:p}`zhaoFastSegmentAnything2023`, TinySAM {cite:p}`shuTinySAMPushingEnvelope2024`, MobileSAM {cite:p}`zhangFasterSegmentAnything2023`, EfficientSAM {cite:p}`xiongEfficientSAMLeveragedMasked2023`, SAM-Lightening {cite:p}`songSAMLighteningLightweightSegment2024`. EfficientSAM utilizes SAMâs masked image pre-training (SAMI) strategy. It employs a MAE-based pre-training method combined with SAM models as a way to obtain high-quality pre-trained ViT encoders. This method is suitable for extracting knowledge from large self-supervised pre-trained SAM models, which in turn generates models that are both lightweight and highly efficient. The described knowledge distillation strategy significantly optimizes the knowledge transfer process from the teacher model to the student model {cite:p}`heMaskedAutoencodersAre2021; baiMaskedAutoencodersEnable2022`.

```{figure} images/ZlKMzlIeGzVP6XxMhiKf-v1.png
:name: cEwjf0su9Q
:align: center
:width: 70%

512Ã512 TEM images and their corresponding ground truth mask images.
```

{numref}`Figure %s <cEwjf0su9Q>` illustrates that the dataset used in this paper. It consists of transmission electron microscopy (TEM) images and their corresponding ground truth masks. The raw images are sliced into subimages of 512 Ã 512 pixels for model training and evaluation. The entire dataset consists of 2000 images, including 1400 for training and 600 for testing. The ground truth mask of each image is manually labeled by hand to ensure the accuracy and reliability of the labeling. Each TEM image is equipped with a corresponding mask, which shows the target nanoparticles as white areas and the non-target areas as black background. These mask images are used for model training during the supervised learning process, and the pairing of high pixel resolution TEM images with accurately labeled true-label masks ensures that the model can learn to distinguish nanoparticles with high accuracy.

For the YOLOv8n-seg model and the VMamba model, data with a resolution of 512x512 was used directly for training. However, the EfficientSAM model is a pre-trained model that requires the size of the input image and output mask to be fixed at 1024x1024. The Swin-UNet model uses images scaled to 448x448, and due to the âShift Windowsâ operation in Swin Transformer, there is a certain limitation on the resolution of the input and output images, which needs to be 224. Therefore, during the training process, the training and test data were adjusted to match the input requirements of the model by adjusting resolution without re-croping the raw images.

# Results

## TEM Image Segmentation

```{list-table} Model's names, and resolutions, throughput, pixels per second, VRAM, test IoU, and test Dice Coefficient of models in this paper.
:header-rows: 1
:name: eRmHsk1Lat

* - Model name

  - Resolution

  - Throughput

  - pixel/s

  - VRAM

  - Test IoU

  - Test Dice Coefficient

* - YOLOv8n-seg

  - 512

  - 80.19/s

  - 21021327/s

  - 1513MB

  - 0.93324

  - 0.96546

* - Swin-UNet

  - 448

  - 64.89/s

  - 13023682/s

  - 1793MB

  - 0.96495

  - 0.98216

* - VMamba

  - 512

  - 16.41/s

  - 4301783/s

  - 1823MB

  - 0.99449

  - 0.99723

* - EfficientSAM-tiny

  - 1024

  - 17.94/s

  - 18811453/s

  - 1827MB

  - 0.99672

  - 0.99836

```

This paper compares four models, YOLOv8n-seg, EfficientSAM-tiny, Swin-UNet, and VMamba. The comparison is analyzed by comparing the accuracy, throughput, number of parameters, and video memory usage.

- YOLOv8n-seg provides a high throughput (80.19/s) with a relatively small number of parameters (3.41M) and video memory of 1513MB. This model, while performing well in terms of accuracy, can be seen in {numref}`Figure %s <Qy3XvUUvyI>` where overfitting occurs during the training of YOLO. Early stopping was used to mitigate this problem.
- Swin-UNet has the sliding window attention mechanism, the number of parameters (3.65M) and video memory usage 1793MB are moderate and the throughput (64.89/s) is high. Swin-UNet based on the sliding window mechanism has a significant advantage in training and inference speed.
- VMamba is based on a new architecture based on Mamba. It has a relatively small number of parameters (3.15M) and a video memory of 1823MB. However, its throughput (16.41/s) is low and its inferencing is slow. It is worth noting that Mamba, being a new architecture, is currently not able to train with multiple cards, unlike the other models.
- EfficientSAM-tiny has a high number of parameters (10.19M) and video memory usage 1827MB, relatively low throughput (17.94/s), but has a significant advantage in accuracy. Despite its high number of parameters, it was the final model chosen due to its excellent accuracy.

```{figure} images/Qmtcihxo7LyYlO6DvqUq-v1.png
:name: Qy3XvUUvyI
:align: center
:width: 100%

The top row of graphs shows detailed zoomed sections of the full range loss curves displayed in the bottom row for various segmentation models for comparison: YOLOv8n-seg, Swin-UNet, VMamba, and EfficientSAM.
```

The training and testing losses of four different segmentation models (YOLOv8n-seg, Swin-UNet, VMamba, and EfficientSAM) are comparatively analyzed in {numref}`Figure %s <Qy3XvUUvyI>`. {numref}`Figure %s <Qy3XvUUvyI>` shows how the loss of each model varies over 1000 epochs. First, the training loss of the YOLOv8n-seg model gradually decreases and stabilizes, while the testing loss significantly increases after the initial fluctuation, indicating a certain degree of overfitting in this model. Second, the Swin-UNet model shows a more consistent downward trend in training and testing losses, and although the testing loss is slightly higher than the training loss, the overall curve tends to be stable, showing good generalization ability. Thirdly, the training and testing loss curves of the VMamba model are very close to each other and drop rapidly in a short period of time, after which they remain at a low level, indicating that it has a significant advantage in convergence speed and stability. Finally, the EfficientSAM model performs particularly well, as its training and testing losses almost completely overlap and are maintained at a very low level throughout the training process, showing extremely high training efficiency and excellent generalization performance.

```{figure} images/aYfIBd8apZhsJE9cjZLZ-v1.png
:name: FnM0Z3oOIl
:align: center
:width: 90%

Comparison of training and test IOU and Dice coefficients for different segmentation models: Swin-UNet, VMamba, and EfficientSAM.
```

{numref}`Figure %s <FnM0Z3oOIl>` shows the trend of training and testing Intersection over Union (IoU) and Dice-SÃ¸rensen coefficient (Dice Coefficient) with 1000 training epochs for three different segmentation models (Swin-UNet, VMamba, EfficientSAM). The training and testing IoU and Dice Coefficient curves for the YOLO model are not available here, but the final IoU and Dice Coefficient are shown in {numref}`Table %s <eRmHsk1Lat>`. For the Swin-UNet model, the training and testing IoU rises rapidly at the beginning of the training rounds, slows down at about round 400. 1000th round to reach an IoU value of about 0.965. The training and testing Dice Coefficient shows a similar trend and eventually stabilizes at about 0.982. Both the training and testing IoU and Dice Coefficient of the VMamba model show a rapid increase with slight fluctuations in the early stages of training. However, after about 300 rounds, these metrics stabilize rapidly and eventually reach about 0.994 and 0.997. This indicates that the VMamba model performs well both in terms of convergence speed and final performance. Notably, the EfficientSAM model performs significantly better than the other two models. Its training and testing IoUs as well as Dice Coefficient rapidly approach 1.0 early in training, which may be due to the use of a pre-trained model. These metrics did not fluctuate significantly during subsequent training, and quickly reached higher accuracy, eventually stabilizing at about 0.997 and 0.998.

```{figure} images/gc9vJoi7RLsyISpB0Msr-v1.png
:name: eTzB6lohnz
:align: center
:width: 100%

Comparison of different segmentation methods. Left: input image and zoomed-in area. Then, segmentation results of Ground Truth, Swin-UNet, VMamba, YOLOv8, and EfficientSAM.
```

The results of Mask comparison of different segmentation methods in detail are shown in {numref}`Figure %s <eTzB6lohnz>`. Firstly, the input image and its magnified region are shown, followed by the segmentation results of âGround Truth Maskâ, Swin-UNet, VMamba, YOLOv8, and EfficientSAM in order. The âGround Truth Maskâ provides the ideal reference mask for comparing the performance of other methods.

The segmentation results of Swin-UNet show a slight lack of edge detail, with some regions failing to be segmented correctly, which is often unacceptable for scientific research. VMamba performs similarly to Swin-UNet, but with a smoother boundary treatment. YOLOv8âs segmentation results have multiple targets boxed and labeled, however, it does not perform as well as the previous two in terms of fine-grained segmentation. It is worth noting that YOLOv8 has a significant advantage in real-time detection and processing speed, which is especially suitable for application scenarios that require a fast response, but the accuracy of the recognition is more important than the speed in scientific research. EfficientSAM performs excellently in preserving the integrity of the target and has a clearer boundary. The EfficientSAM and YOLOv8-seg models segmentation can provide IDs (different colors correspond to different IDs). EfficientSAM can generate masks with IDs by calling the mask decoder multiple times with an array of point prompts, and YOLOv8-seg can also provide IDs because it is a target detection model that performs segmentation after single term of detection.

## TEM Video Tracking

```{figure} images/wYwdA502ujaaB7293m5M-v1.png
:name: BJo1hhWS1b
:align: center
:width: 100%

Comparison of key video frames. The first row shows original frames, the second row shows segmentation results, and the third row is a magnified view of the red box area, showing segmentation and tracking of objects.
```

In this paper, a video analysis of the sintering process of the material at a high temperature of 800â is carried out. Figure {numref}`Figure %s <BJo1hhWS1b>` shows the original images of the key frames and the segmentation results. The first row shows the original frames, showing the continuous image of some key frames 1-115. The second row shows the segmentation result from EfficientSAM, which shows the change of objects on the time axis by color marking different objects. The third row shows a magnified view of the red-framed area, in order to show the segmentation and tracking process more clearly. The images from frame 77-115 show the sintering phenomenon of multiple nano-particles, especially between frame 111-115, where the three nano-particles are gradually fused and show obvious morphological changes. This indicates that a significant sintering process occurred in the material at a high temperature of 800â. Comparison of the segmentation results with the original frames shows that the method in this paper is able to accurately identify and label the objects, and track them effectively even when they become sintered. The color-marked segmentation results clearly show the dynamic changes of different particles during the sintering process, and track the evolution of the microstructure of the material under the high-temperature environment very well.

# Discussion

## Segmentation and Tracking

The models released for computer vision are often based on realistic images, which are often ineffective when applied directly to microscope data for materials research. This paper takes microscope datasets and trains or fine-tunes them on different models. Deep learning shows the superior performance of the models in microscope image segmentation. All four segmentation models perform well on both the training and test sets, especially the EfficientSAM model, which shows the highest stability and accuracy on all evaluation metrics, indicating its strong generalization ability and robustness in the segmentation task. EfficientSAM demonstrates better segmentation performance in the application scenario {numref}`Figure %s <eTzB6lohnz>`, and the edges are closer to the ground truth, according to the results in {numref}`Table %s <eRmHsk1Lat>`, EfficientSAM-tiny does have an obvious advantage in terms of accuracy. Although the other models have their own characteristics in terms of throughput, number of parameters and memory usage, EfficientSAM-tiny outperforms the other models in terms of accuracy, with an IoU of 0.99672 and a Dice Coefficient of 0.99836. At the same time, the performance of EfficientSAM model is better than the other models in terms of loss in both the training and the testing curves, showing optimal segmentation performance and generalization ability. These results suggest that the EfficientSAM model may be a potentially superior choice for handling segmentation tasks with greater efficiency and effectiveness. This provides an important reference for the selection and optimization of the tracking model.

For the autoencoder architecture, the encoder part of the network drastically reduces the resolution of the feature maps through the pooling layer, which is not conducive to generating accurate segmentation masks. Skip-connection in the Swin-UNet can introduce high-resolution features from the shallow convolutional layers, which contain rich low-level information to help generate better segmentation masks. However, the edges of the mask after Swin-UNet segmentation are still not clean enough in {numref}`Figure %s <eTzB6lohnz>`. The algorithms in the tracking part are able to maintain high performance when dealing with the kinematic complexities such as particle growth, and motion during sintering, demonstrating the dynamics of the particles. This indicates the tracking method of this paper has significant application potential. In particular, the outstanding EfficientSAM and the efficiency of the DeAOT model demonstrate the potential of deep learning techniques in microscopy image/video analysis.

## Future Directions

In {numref}`Figure %s <Qy3XvUUvyI>`, the training loss of the YOLO model decreases initially, but the test loss picks up after reaching its lowest point in the 580 epoch and shows a U-shaped curve. This indicates that the model is overfitting. This may be due to the fact that a model of this size is not able to capture the visual information in the image well. However, this does not mean that the model is not useful. The advantage of YOLO is the speed of inference. In future research, YOLO can be used to perform initial segmentation at high speed to obtain positional information, and then SAM could be used to re-segment the critical parts. This will increase the speed of inference while still maintaining high accuracy. In addition, in higher pixel resolution microscope videos, it may be necessary to cut the image into small regions for tracking, which may significantly slow down the inference speed of the model. In this case, YOLO can be used to perform overall fast target detection on the scaled resolution image, and then the high pixel resolution target region can be cropped, based on the detection result. Thereafter, SAM can be used to obtain more accurate segmentation.

```{figure} images/4dZsrUFpKe1CP9iui5XU-v1.png
:name: KhF5puVr6V
:align: center
:width: 100%

The original image and the segmentation results from LISA model obtained by progressively optimized prompts.
```

In the future, there are potential applications of large language modeling (LLM) in microscopy image analysis. For example, the "Large Language Instructed Segmentation Assistant" (LISA) model is fine-tuned with a multimodal large language model to reason and generate âtokensâ with approximate location information and combined with the SAM model for accurate segmentation {cite:p}`laiLISAReasoningSegmentation2024`. However, the LISA model still has some limitations when processing microscopy images. Although the segmentation level can be improved by optimizing the prompt, as shown in top of {numref}`Figure %s <KhF5puVr6V>`, although most of the nanoparticles are correctly labeled, there are still some particles that are missed or incorrectly labeled. The main reason for this is that the training data is mainly from real-world scenarios rather than specialized microscope images. Perhaps incorporating microscope images into the training data could help improve the modelâs performance in microscope images. In another study {cite:p}`zhangSamGuidedEnhancedFineGrained2023` on sam-guided multimodal LLM, the SAM visual coder was simultaneously introduced into a multimodal large language model. By adding a SAM visual encoder (for detail information) to the CLIP visual encoder (for global information) more detailed characterization can be achieved. Using multiple visual encoders with different functions can significantly improve the accuracy and detail of the generated image descriptions {cite:p}`zhangSamGuidedEnhancedFineGrained2023`. Visual encoders with different functions are similar to the compound eyes of an insect and can provide information with different details: the CLIP visual encoder provides the overall category and information of the image, while the SAM visual encoder provides information about the edges and shapes of the objects. In the future, by combining the multimodal LLM model with the SAM model, it is expected to further enhance the ability to analyze and reason about microscope videos.

# Conclusion

This paper demonstrates an approach to microscopy image segmentation and tracking using advanced deep learning models. The superior performance of EfficientSAM-tiny in microscope image segmentation is verified by comparing four advanced deep learning models (Swin-UNet, VMamba, YOLOv8-seg, and EfficientSAM-tiny), which achieve an IoU of 0.99672 and a Dice Coefficient 0.99836.

In the tracking task, EfficientSAM-tiny and DeAOT were combined in the time dimension to efficiently track and identify the nano-particles changes during high temperature sintering. The results of the study show that deep learning techniques have advantages in microscopy image/video analysis, and these neural network-based models have considerable inference speed and accuracy. Furthermore, these approaches are shown to be superior to traditional methods, which are inefficient and require significant human intervention.

Despite the progress made in this paper, there are still some limitations. First, the training and testing data of the models are mainly derived from a single phase. This implies that fine-tuning of the models may be required to adapt to microscopy images containing multiple phases. Second, from the data shown, the model may maintain the same tracking ID even after particles have sintered together: this can complicate analyses that are interested in understanding when this phenomenon initiates. These limitations suggest optimization directions for future research. The potential for more complex analysis of microscopy videos is expected to be realized in the future through the introduction of new techniques such as multimodal large language models, and introducing diversity to the dataset.

# Acknowledgements

_This work was carried out in part at the Singh Center for Nanotechnology, which is supported by the NSF National Nanotechnology Coordinated Infrastructure Program under grant NNCI-2025608 and through the use of facilities supported by the University of Pennsylvania Materials Research Science and Engineering Center (MRSEC) DMR-2309043. C.-Y. C. and E.A.S. acknowledge additional support through the NSF Division of Materials Research's Metals and Metallic Nanostructures program, DMR-2303084._

# Declaration of Generative AI in Scientific Writing:

AI tools (GPT-4o) were used to improve the grammar and readability of this manuscript.
